{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload PurpleAir Backlog to ESDR\n",
    "\n",
    "This script uploads PurpleAir data files which are *newer than* START_FILE.  Useful for running side-by-side with the realtime uploader, to clean up gaps in the mirrored data left behind by the realtime uploader's frequent failures.\n",
    "\n",
    "IMPORTANT: The script has *no* concept of an ending data file, so it'll keep running until it catches up to the realtime mirror and will thus be doing duplicate work.  It is thus 100% UP TO YOU to stop the script when the gap in the mirror is fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_FILE = '20230804/000000utc.json.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2, html, json, os, re, requests, subprocess, sys, dateutil, datetime, glob, stat, math\n",
    "\n",
    "from dateutil import rrule, tz, parser\n",
    "from sqlitedict import SqliteDict\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate to load utils.ipynb\n",
    "# See https://github.com/CMU-CREATE-Lab/python-utils/blob/master/utils.ipynb\n",
    "\n",
    "\n",
    "def exec_ipynb(filename_or_url):\n",
    "    nb = (requests.get(filename_or_url).json() if re.match(r'https?:', filename_or_url) else json.load(open(filename_or_url)))\n",
    "    if(nb['nbformat'] >= 4):\n",
    "        src = [''.join(cell['source']) for cell in nb['cells'] if cell['cell_type'] == 'code']\n",
    "    else:\n",
    "        src = [''.join(cell['input']) for cell in nb['worksheets'][0]['cells'] if cell['cell_type'] == 'code']\n",
    "\n",
    "    tmpname = '/tmp/%s-%s-%d.py' % (os.path.basename(filename_or_url),\n",
    "                                    datetime.datetime.now().strftime('%Y%m%d%H%M%S%f'),\n",
    "                                    os.getpid())\n",
    "    src = '\\n\\n\\n'.join(src)\n",
    "    open(tmpname, 'w').write(src)\n",
    "    code = compile(src, tmpname, 'exec')\n",
    "    exec(code, globals())\n",
    "\n",
    "\n",
    "exec_ipynb('./python-utils/utils.ipynb')\n",
    "exec_ipynb('./python-utils/esdr-library.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAT_SERVICE_NAME = 'Purpleair backlog uploader (20230804 - 20230811)'\n",
    "STAT_HOSTNAME = 'hal21'\n",
    "STAT_SHORTNAME = 'purpleair-upload-to-esdr-backlog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stat.set_service(STAT_SERVICE_NAME)\n",
    "progress = SqliteDict('upload-purpleair-backlog.sqlite', autocommit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PURPLE_AIR_V2_ESDR_PRODUCT_NAME = 'purpleair_v2' # https://esdr.cmucreatelab.org/api/v1/products/purpleair_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First time uploading, create a new client like so:\n",
    "\n",
    "# Esdr.save_client('esdr-auth-purpleair-uploader.json', 'PurpleAir uploader for timemachine1')\n",
    "\n",
    "# and then follow the directions it prints, which include visiting esdr.cmucreatelab.org and creating\n",
    "# a client with given parameters, and also editing esdr-auth-baaqm-uploader.json to include your\n",
    "# username and password\n",
    "\n",
    "# Do not add esdr-auth-*.json to the git repo\n",
    "# !echo 'esdr-auth-*.json' >>.gitignore\n",
    "\n",
    "esdr = Esdr('esdr-auth-purpleair-uploader.json')\n",
    "\n",
    "# load the PurpleAir v2 product\n",
    "purpleair_product = esdr.get_product_by_name(PURPLE_AIR_V2_ESDR_PRODUCT_NAME)\n",
    "\n",
    "feed_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#\n",
    "# Parse and convert device records to ESDR\n",
    "#\n",
    "\n",
    "def computeLatLon(deviceRecord):\n",
    "    return (float(deviceRecord['latitude']), float(deviceRecord['longitude']))\n",
    "\n",
    "def computeEsdrId(deviceRecord):\n",
    "    lat, lon = computeLatLon(deviceRecord)\n",
    "\n",
    "    id = \"%s_%06d%s%06d%s\" % (deviceRecord['sensor_index'], \n",
    "                              round(1000 * abs(lat)), 'NS'[lat < 0], \n",
    "                              round(1000 * abs(lon)), 'EW'[lon < 0])\n",
    "    return id.replace('.','_')\n",
    "\n",
    "def computeEpochTimestamp(deviceRecord):\n",
    "    try:\n",
    "        return deviceRecord['last_seen']\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def computeEsdrRecord(deviceRecord):\n",
    "    data = {}\n",
    "\n",
    "    data['time'] = computeEpochTimestamp(deviceRecord)\n",
    "\n",
    "    # As of 2022-06-02, we're only mirroring the following\n",
    "    channelNames = ['voc', 'ozone1', 'humidity', 'pressure', 'temperature', 'pm2.5', 'pm2.5_a', 'pm2.5_b']\n",
    "\n",
    "    for key in channelNames:\n",
    "        translated_key = key\n",
    "        if key == \"ozone1\":\n",
    "            translated_key = \"ozone\"\n",
    "        elif key == \"pm2.5\":\n",
    "            translated_key = \"PM2_5\"\n",
    "        elif key == \"pm2.5_a\":\n",
    "            translated_key = \"PM2_5_a\"\n",
    "        elif key == \"pm2.5_b\":\n",
    "            translated_key = \"PM2_5_b\"\n",
    "        elif key == \"temperature\":\n",
    "            translated_key = \"temp_f\"\n",
    "            \n",
    "        if key in deviceRecord:\n",
    "            try:\n",
    "                val = float(deviceRecord[key])\n",
    "                if not math.isnan(val):\n",
    "                    data[translated_key] = float(deviceRecord[key])\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to parse '{key}' value as float for ID {deviceRecord['sensor_index']}: {repr(e)}\")\n",
    "                pass\n",
    "        #else:\n",
    "        #    print(f\"Expected channel '{key}' not found for ID {deviceRecord['ID']}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def computeEsdrName(deviceRecord):\n",
    "    if 'name' in deviceRecord:\n",
    "        return deviceRecord['name'].strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "###########################################################\n",
    "#\n",
    "# Accumulate deviceRecords from multiple JSON input files\n",
    "#\n",
    "\n",
    "def accumulateReset():\n",
    "    global accumulator\n",
    "    accumulator = defaultdict(lambda: {'records':defaultdict(lambda: {})})\n",
    "\n",
    "def accumulateJson(path):\n",
    "    nUploads = 0\n",
    "    nonameCount = 0\n",
    "    locationCounts = defaultdict(lambda:0)\n",
    "    try:\n",
    "        js = json.load(bz2.open(path, 'r'))\n",
    "    except:\n",
    "        Stat.warning(f\"Error parsing file '{path}'\", host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "        return\n",
    "    fields = js['fields']\n",
    "    data = js['data']\n",
    "    records = pd.DataFrame(data, columns =fields).to_dict('records')\n",
    "\n",
    "    for deviceRecord in records:\n",
    "        if math.isnan(deviceRecord['sensor_index']):\n",
    "             locationCounts['noID'] += 1\n",
    "             continue\n",
    "\n",
    "        if math.isnan(deviceRecord['latitude']) or math.isnan(deviceRecord['longitude']):\n",
    "            locationCounts['noLatLon'] += 1\n",
    "            continue\n",
    "\n",
    "        epochTimestamp = computeEpochTimestamp(deviceRecord)\n",
    "        if not epochTimestamp:\n",
    "            locationCounts['noTimestamp'] += 1\n",
    "            continue\n",
    "\n",
    "        location = \"outdoor\" if deviceRecord['location_type'] == 0 else \"indoor\"\n",
    "        locationCounts[location] += 1\n",
    "        #if location == 'indoor':\n",
    "        #    continue\n",
    "\n",
    "        purpleAirId = deviceRecord['sensor_index']\n",
    "\n",
    "        record = computeEsdrRecord(deviceRecord)\n",
    "        name = computeEsdrName(deviceRecord)\n",
    "        if not name:\n",
    "            nonameCount += 1\n",
    "            continue\n",
    "\n",
    "        if not isinstance(purpleAirId, int):\n",
    "            continue\n",
    "        \n",
    "        a = accumulator[purpleAirId]\n",
    "\n",
    "        # create a bucket for this timestamp if we haven't seen it before\n",
    "        if epochTimestamp not in a['records']:\n",
    "            a['records'][epochTimestamp] = {}\n",
    "\n",
    "        # copy the keys for this record into this timestamp's bucket.  Need\n",
    "        # to do this to handle device A and B for the same timestamp.\n",
    "        for key in record.keys():\n",
    "            a['records'][epochTimestamp][key] = record[key]\n",
    "\n",
    "    \n",
    "        a['name'] = name\n",
    "        lat, lon = computeLatLon(deviceRecord)\n",
    "        a['lat'] = lat\n",
    "        a['lon'] = lon\n",
    "        a['exposure'] = location\n",
    "        \n",
    "        nUploads += 1\n",
    "\n",
    "    Stat.info('%s: using %d of %d records %s' % (path, nUploads, len(records), json.dumps(locationCounts)), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "    Stat.info('After merge, total of %d ESDR IDs' % len(accumulator), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "    if nonameCount:\n",
    "        Stat.warning('%d purpleair records had no Label field and could not be merged' % nonameCount, host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "############################################\n",
    "#\n",
    "# Find JSON files we haven't processed yet\n",
    "#\n",
    "\n",
    "def pathTopDir(path):\n",
    "    return path.split('/')[0]\n",
    "\n",
    "def pathSansTopDir(path):\n",
    "    tokens = path.split('/')\n",
    "    return '/'.join(tokens[1:])\n",
    "\n",
    "# Get newer files than path in dir\n",
    "def getNewerFiles(dir, newerThan, nSubdirLevels):\n",
    "    ret = []\n",
    "    if nSubdirLevels:\n",
    "        for subdir in sorted(os.listdir(dir)):\n",
    "            if not newerThan or subdir > pathTopDir(newerThan):\n",
    "                for file in getNewerFiles(dir + '/' + subdir, None, nSubdirLevels-1):\n",
    "                    ret.append(subdir + '/' + file)\n",
    "            elif subdir == pathTopDir(newerThan):\n",
    "                for file in getNewerFiles(dir + '/' + subdir, pathSansTopDir(newerThan), nSubdirLevels-1):\n",
    "                    ret.append(subdir + '/' + file)\n",
    "    else:\n",
    "        for file in sorted(os.listdir(dir)):\n",
    "            if not newerThan or file > newerThan:\n",
    "                ret.append(file)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_from_cache_or_create_it(device_serial_number, name, lat, lon, exposure):\n",
    "    global feed_cache, esdr, purpleair_product\n",
    "\n",
    "    cache_key = (device_serial_number, lat, lon)\n",
    "\n",
    "    if not (cache_key in feed_cache):\n",
    "        device = esdr.get_or_create_device(purpleair_product, str(device_serial_number))\n",
    "        feed_name = name + ' ' + purpleair_product['prettyName']\n",
    "        feed = esdr.get_or_create_feed_with_name(device, feed_name, lat, lon, exposure)\n",
    "        feed_id = feed['id']\n",
    "        # If the name of a PurpeAir has changed since last upload, change it\n",
    "        if (feed['name'] != feed_name):\n",
    "            old_name = feed['name']\n",
    "            esdr.update_feed_name(feed_id, feed_name)\n",
    "            feed['name'] = feed_name\n",
    "            Stat.info(f\"Field 'name' for feed '{feed_id}' changed from '{old_name}' to '{feed_name}'\", host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "        # If the exposure field (location in/out) of a PurpleAir has changed since last upload, change it\n",
    "        if (feed['exposure'] != exposure):\n",
    "            old_exposure = feed['exposure']\n",
    "            esdr.update_feed_exposure(feed_id, exposure)\n",
    "            feed['exposure'] = exposure\n",
    "            Stat.info(f\"Field 'exposure' for feed '{feed_id}' changed from '{old_exposure}' to '{exposure}'\", host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "        feed_cache[cache_key] = feed\n",
    "        #print(f\"cached feed for device {device_serial_number} at ({lat},{lon})\")\n",
    "\n",
    "    return feed_cache[cache_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#\n",
    "# Upload to ESDR\n",
    "#\n",
    "\n",
    "NUM_UPLOADS_IN_PARALLEL = 6\n",
    "\n",
    "def sortedDict(dict):\n",
    "    return {k:dict[k] for k in sorted(dict.keys()) }\n",
    "\n",
    "def uploadId(id):\n",
    "    try:\n",
    "        a = accumulator[id]\n",
    "        if 'lat' in a and 'lon' in a:\n",
    "            feed = get_feed_from_cache_or_create_it(id, a['name'], a['lat'], a['lon'], a['exposure'])\n",
    "            dicts = list(sortedDict(a['records']).values())\n",
    "            # df = pd.DataFrame(dicts)\n",
    "            esdr.upload_dicts(feed, dicts)\n",
    "            print('Successfully uploaded PurpleAir ID %d' % (id))\n",
    "            #Stat.info('Successfully uploaded PurpleAir ID %d' % (id), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "        else:\n",
    "            # print('Skipping ID %s because it has no lat/lon' % (id))\n",
    "            Stat.warning('Skipping ID %s because it has no lat/lon' % (id), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "    except requests.HTTPError as e:\n",
    "        # print('Failed to upload to feed corresponding to ID %s (HTTP %d)' % (id, e.response.status_code))\n",
    "        Stat.warning('Failed to upload to feed corresponding to ID %s (HTTP %d)' % (id, e.response.status_code), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "    except:\n",
    "        # print('Failed to upload to feed corresponding to ID %s' % (id))\n",
    "        Stat.warning('Failed to upload to feed corresponding to ID %s' % (id), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "    sys.stdout.flush()\n",
    "    sys.stderr.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = esdr.get_device_by_serial_number(purpleair_product, str(79177))\n",
    "# print(device)\n",
    "# feed = esdr.get_feed(device=device, lat=float(\"37.258984\"), lon=float(\"-121.95168\"), order_by='-id')\n",
    "# print(feed)\n",
    "# print()\n",
    "# if (feed['exposure'] != \"outdoor\"):\n",
    "#   resp = esdr.update_feed_exposure(feed['id'], \"outdoor\")\n",
    "#   print(resp)\n",
    "# print()\n",
    "# feed = esdr.get_feed(device=device, lat=float(\"37.258984\"), lon=float(\"-121.95168\"), order_by='-id')\n",
    "# print(feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed_ids = []\n",
    "\n",
    "# for id in feed_ids:\n",
    "#      response = esdr.delete_feed_by_id(id)\n",
    "#      print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this 20 times and then exit, relying on the cron job to start it up again. We do this as a simple way to deal with\n",
    "# refreshing the ESDR OAuth2 token, rather than adding code to catch the HTTP 401/403 error upon upload, yada yada. The\n",
    "# drawback of doing it this way is that the get_feed_from_cache_or_create_it() function has to start fresh\n",
    "# with a new cache, but doing so once every 20 runs is fine.\n",
    "for i in list(range(20)):\n",
    "    lastUploaded = progress.get('lastUploaded', START_FILE)\n",
    "\n",
    "    Stat.info(\"Mirroring PurpleAir backlog of data files newer than %s.  Last uploaded = %s\" % (START_FILE, lastUploaded), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "    files = getNewerFiles('mirror2', lastUploaded, nSubdirLevels=1)\n",
    "\n",
    "    # Only json.bz2 files\n",
    "    files = list(filter(re.compile(r'\\.json\\.bz2$').search, files))\n",
    "\n",
    "    # A maximum of the first 720 files\n",
    "    files = files[0:720]\n",
    "\n",
    "    accumulateReset()\n",
    "\n",
    "    for file in files:\n",
    "        accumulateJson('mirror2/' + file)\n",
    "\n",
    "    # for id in sorted(accumulator.keys()):\n",
    "    #     uploadId(id)\n",
    "\n",
    "    # Multithreading support\n",
    "    with ThreadPoolExecutor(NUM_UPLOADS_IN_PARALLEL) as pool:\n",
    "        pool.map(uploadId, sorted(accumulator.keys()))\n",
    "\n",
    "    Stat.up('Uploaded %d devices from %d files (%s ... %s)' % (len(accumulator), len(files), files[0], files[-1]), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME,\n",
    "            valid_for_secs=3600 * 4)\n",
    "    progress['lastUploaded'] = files[-1]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88ccf14db98d8c5ffc7dfc032f8885d6bd103005d67bc8f7e90175b13672a1c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3 (default, Mar 27 2019, 22:11:17) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
