{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, re, requests, subprocess, sys, datetime, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate to load utils.ipynb\n",
    "# See https://github.com/CMU-CREATE-Lab/python-utils/blob/master/utils.ipynb\n",
    "\n",
    "\n",
    "def exec_ipynb(filename_or_url):\n",
    "    nb = (requests.get(filename_or_url).json() if re.match(r'https?:', filename_or_url) else json.load(open(filename_or_url)))\n",
    "    if(nb['nbformat'] >= 4):\n",
    "        src = [''.join(cell['source']) for cell in nb['cells'] if cell['cell_type'] == 'code']\n",
    "    else:\n",
    "        src = [''.join(cell['input']) for cell in nb['worksheets'][0]['cells'] if cell['cell_type'] == 'code']\n",
    "\n",
    "    tmpname = '/tmp/%s-%s-%d.py' % (os.path.basename(filename_or_url),\n",
    "                                    datetime.datetime.now().strftime('%Y%m%d%H%M%S%f'),\n",
    "                                    os.getpid())\n",
    "    src = '\\n\\n\\n'.join(src)\n",
    "    open(tmpname, 'w').write(src)\n",
    "    code = compile(src, tmpname, 'exec')\n",
    "    exec(code, globals())\n",
    "\n",
    "\n",
    "exec_ipynb('./python-utils/utils.ipynb')\n",
    "exec_ipynb('./python-utils/esdr-library.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PURPLE_AIR_V2_ESDR_PRODUCT_NAME = 'purpleair_v2' # https://esdr.cmucreatelab.org/api/v1/products/purpleair_v2\n",
    "\n",
    "# First time uploading, create a new client like so:\n",
    "\n",
    "# Esdr.save_client('esdr-auth-purpleair-uploader.json', 'PurpleAir uploader for timemachine1')\n",
    "\n",
    "# and then follow the directions it prints, which include visiting esdr.cmucreatelab.org and creating\n",
    "# a client with given parameters, and also editing esdr-auth-baaqm-uploader.json to include your\n",
    "# username and password\n",
    "\n",
    "# Do not add esdr-auth-*.json to the git repo\n",
    "# !echo 'esdr-auth-*.json' >>.gitignore\n",
    "\n",
    "esdr = Esdr('esdr-auth-purpleair-uploader.json')\n",
    "\n",
    "# load the PurpleAir v2 product\n",
    "purpleair_product = esdr.get_product_by_name(PURPLE_AIR_V2_ESDR_PRODUCT_NAME)\n",
    "\n",
    "feed_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find pre-2019 PurpleAirs\n",
    "\n",
    "epoch = \"1569888000\"\n",
    "url = f\"https://esdr.cmucreatelab.org/api/v1/feeds?whereAnd=productId=69,minTimeSecs%3C={epoch}\"\n",
    "response = requests.get(url, timeout=120)\n",
    "response_data = response.json()\n",
    "count = response_data['data']['totalCount']\n",
    "num_offsets = math.ceil(count / 1000)\n",
    "sensors = response_data['data']['rows']\n",
    "for offset_num in range(num_offsets):\n",
    "    url = f\"https://esdr.cmucreatelab.org/api/v1/feeds?whereAnd=productId=69,minTimeSecs%3C={epoch}&offset={(offset_num + 1) * 1000}\"\n",
    "    print(url)\n",
    "    response = requests.get(url, timeout=120)\n",
    "    response_data = response.json()\n",
    "    sensors += response_data['data']['rows']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link pre-2019 A&B PurpleAirs together\n",
    "\n",
    "dict = {}\n",
    "count_match = 0\n",
    "fixed_channels = set(['PM2_5', 'PM2_5_a', 'PM2_5_b', 'humidity', 'pressure', 'temp_f'])\n",
    "for sensor in sensors:\n",
    "    matched = False\n",
    "\n",
    "    # Skip garbage ids\n",
    "    if sensor['id'] in [25740,25657,24741,24614,32844,21831,53721,53722,15007,15798]:\n",
    "        continue\n",
    "\n",
    "    for d in dict.values():\n",
    "        if 'longitude' in d and d['longitude'] == sensor['longitude'] and d['latitude'] == sensor['latitude']:\n",
    "            matched = True\n",
    "            matches = []\n",
    "            if 'matches' in d:\n",
    "                matches = d['matches']\n",
    "                filtered_channels = list(fixed_channels & set(list(sensor['channelBounds']['channels'].keys())))\n",
    "                matches[sensor['id']] = {'name' : sensor['name'], 'channels' : filtered_channels}\n",
    "                dict[d['id']]['matches'] = matches\n",
    "                break\n",
    "\n",
    "    if not matched:\n",
    "        filtered_channels = list(fixed_channels & set(list(sensor['channelBounds']['channels'].keys())))\n",
    "        channels = filtered_channels\n",
    "        dict[sensor['id']] = {'matches' : {}, 'id': sensor['id'], 'name': sensor['name'], 'channels' : channels, 'latitude': sensor['latitude'], 'longitude' : sensor['longitude'], 'exposure' : sensor['exposure']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete pairings that matched more than 2 sensors at the same lat/lon. ~130 are removed. \n",
    "print(len(dict.keys()))\n",
    "for sensor in list(dict.values()):\n",
    "    if (len(sensor['matches'].keys()) > 2):\n",
    "        del dict[sensor['id']]\n",
    "print(len(dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match v1 ids to new v2 ids\n",
    "\n",
    "# Only 1 match means just channelA\n",
    "# matches may not have equal channels amongst each other, so we need to do the intersection of channel names\n",
    "\n",
    "v2_pairings = {}\n",
    "\n",
    "for sensor in dict.values():\n",
    "    response = esdr.api('GET', '/api/v1/feeds', {'whereAnd': 'latitude=%s,longitude=%s,productId=%s' % (sensor['latitude'], sensor['longitude'], '101')})\n",
    "    #response = esdr.api('GET', '/api/v1/feeds', {'whereAnd': 'name=%s,productId=%s' % (re.sub(\"\\s+B\\s+\", \" \", sensor['name']), '101')})\n",
    "    if len(response['data']['rows']) > 0:\n",
    "        pairing = {response['data']['rows'][0]['id'] : [sensor['id']] + list(sensor['matches'].keys())}\n",
    "        v2_pairings[response['data']['rows'][0]['id']] = [sensor['id']] + list(sensor['matches'].keys())\n",
    "        print(pairing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2_pairings_flattened = [item for sublist in list(v2_pairings.values()) for item in sublist]\n",
    "# print(v2_pairings_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "\n",
    "data_list = []\n",
    "count = 0\n",
    "epoch = '1569888000'\n",
    "for sensor in dict.values():\n",
    "    if sensor['id'] not in v2_pairings_flattened:\n",
    "        continue\n",
    "    data = {'a' : {}, 'b' : {}, 'avg': []}\n",
    "    ids = [sensor['id']] + list(sensor['matches'].keys())\n",
    "    if (len(ids) == 2):\n",
    "        channels = {ids[0] : dict[ids[0]]['channels'], ids[1] : dict[ids[0]]['matches'][ids[1]]['channels']}\n",
    "    else:\n",
    "        channels = {ids[0] : dict[ids[0]]['channels']}\n",
    "        \n",
    "    for idx, id in enumerate(ids):\n",
    "        channels_lookup = \"\"\n",
    "        for channel in channels[id]:\n",
    "            channels_lookup += str(id) + \".\" + channel + \",\"\n",
    "        url = f\"https://esdr.cmucreatelab.org/api/v1/feeds/export/{channels_lookup}?to={epoch}&format=json\"\n",
    "        print(url)\n",
    "        response = requests.get(url, timeout=120)\n",
    "        \n",
    "        # Ignore ones with < 20 data points\n",
    "        if len(response.json()['data']) < 20:\n",
    "            print(f\"Found less than 20 data points for sensor {id}\")\n",
    "            continue\n",
    "\n",
    "        if sensor['id'] == id:\n",
    "            name = sensor['name']\n",
    "        elif id in sensor['matches']:\n",
    "            name = sensor['matches'][id]['name']\n",
    "\n",
    "        if ' B ' in name:\n",
    "            data['b'][id] = response.json()\n",
    "        else:\n",
    "            if idx > 0 and len(data['a'].keys()) > 0:\n",
    "                data['b'][id] = response.json()\n",
    "            else:\n",
    "                data['a'][id] = response.json()\n",
    "\n",
    "    if len(data['a'].keys()) > 0 or len(data['b'].keys()) > 0:\n",
    "        data_list.append(data)\n",
    "    else:\n",
    "        print(\"skipping add\")\n",
    "        \n",
    "    # count += 1\n",
    "    # if count == 1:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# print(len(data_dict[12674]['data']))\n",
    "# print(len(data_dict[12673]['data']))\n",
    "# print(len(dict.keys()))\n",
    "for idx, exports in enumerate(data_list):\n",
    "    print(f\"Processing {idx+1} out of {len(data_list)}\")\n",
    "    pairs = []\n",
    "    if len(exports['a'].keys()) > 0:\n",
    "        pairs = [list(exports['a'].keys())[0]] + pairs\n",
    "    if len(exports['b'].keys()) > 0:\n",
    "        pairs.append(list(exports['b'].keys())[0])\n",
    "    # pairs = [list(exports['a'].keys())[0],list(exports['b'].keys())[0]]\n",
    "    if len(pairs) == 2:\n",
    "        a_channels = [element.split(\".\")[-1] for element in exports['a'][pairs[0]]['channel_names']]\n",
    "        df_channels = ['epoch'] + a_channels\n",
    "        df = pd.DataFrame(exports['a'][pairs[0]]['data'], columns=df_channels).sort_values(by=['epoch'], ascending=True)\n",
    "        \n",
    "        b_channels = [element.split(\".\")[-1] for element in exports['b'][pairs[1]]['channel_names']]\n",
    "        df2_channels = ['epoch'] + b_channels\n",
    "        df2 = pd.DataFrame(exports['b'][pairs[1]]['data'], columns=df2_channels).sort_values(by=['epoch'], ascending=True)\n",
    "\n",
    "\n",
    "        # Find intersection of channels\n",
    "        # Not relevant if you ensure channels pulled from ESDR will have data, otherwise ESDR will create the union and fill in with Nones\n",
    "        a = set(df_channels)\n",
    "        b = set(df2_channels)\n",
    "        channel_intersection = list(a & b)\n",
    "\n",
    "        cols_to_remove = [col for col in df.columns if col not in channel_intersection]\n",
    "        df = df.drop(cols_to_remove, axis=1)\n",
    "        cols_to_remove = [col for col in df2.columns if col not in channel_intersection]\n",
    "        df2 = df2.drop(cols_to_remove, axis=1)\n",
    "\n",
    "        # Compute averages between the two matched sensors\n",
    "        counter = 0\n",
    "        new_data = []\n",
    "        ## for epoch in df2['epoch']:\n",
    "        ##     #found_idx = df['epoch'].sub(epoch).abs().idxmin()\n",
    "        ##     found_idx = abs(df['epoch'] - epoch).idxmin()\n",
    "        ##     # print(f\"for time {epoch} found closest match at index {found_idx}\")\n",
    "\n",
    "        ##     # df.iloc[found_idx]\n",
    "        ##     # df2.iloc[counter]\n",
    "        ##     # for val in df2.iloc[counter]\n",
    "\n",
    "        ##     new_data.append(np.nanmean(np.array([df.iloc[found_idx], df2.iloc[counter]]), axis=0).tolist())\n",
    "        ##     counter += 1\n",
    "\n",
    "        # avg_df_df2 = pd.DataFrame(new_data, columns=df.columns)\n",
    "        \n",
    "        final_cols = df.columns.tolist()\n",
    "        final_cols.remove(\"epoch\")\n",
    "        ##exports['avg'] = {\"channel_names\":final_cols,\"data\":new_data} #pd.DataFrame(new_data, columns=df.columns)\n",
    "\n",
    "        cols_to_remove = [col for col in df.columns if col not in ['epoch', 'PM2_5']]\n",
    "        df_a = df.drop(cols_to_remove, axis=1)\n",
    "        cols_to_remove = [col for col in df2.columns if col not in ['epoch', 'PM2_5']]\n",
    "        df2_b = df2.drop(cols_to_remove, axis=1)\n",
    "\n",
    "        exports['a'] = {pairs[0] : {\"channel_names\":['PM2_5_a'],\"data\":df_a.values.tolist()}}\n",
    "        exports['b'] = {pairs[1] : {\"channel_names\":['PM2_5_b'],\"data\":df2_b.values.tolist()}}\n",
    "\n",
    "        #print(df.iloc[3])\n",
    "\n",
    "        #data = np.array([df.iloc[3], df2.iloc[2]])\n",
    "        #print(np.average(data, axis=0).tolist())\n",
    "\n",
    "        # df[:idx].groupby(df.index[:idx] // 2).mean()\n",
    "        #print(df.iloc[3].groupby(df2.iloc[2] // 2).mean())\n",
    "\n",
    "        #for idx, data in enumerate(data_dict[12668]['data']):\n",
    "        #    print(abs(data[0] - data_dict[12777]['data'][idx][0]))\n",
    "    else:\n",
    "        print(f\"Exports length was not 2 but was instead {len(pairs)} for {exports['a'].keys(), exports['b'].keys()}\")\n",
    "\n",
    "        # Somehow we sometimes get something shoved in b when it should be in a....\n",
    "        tmp = exports['a']\n",
    "        if len(exports['a'].keys()) == 0:\n",
    "            tmp = exports['b']\n",
    "        a_channels = [element.split(\".\")[-1] for element in tmp[pairs[0]]['channel_names']]\n",
    "        df_channels = ['epoch'] + a_channels\n",
    "        df = pd.DataFrame(tmp[pairs[0]]['data'], columns=df_channels).sort_values(by=['epoch'], ascending=True)\n",
    "\n",
    "        cols_to_remove = [col for col in df.columns if col not in ['epoch', 'PM2_5']]\n",
    "        df_a = df.drop(cols_to_remove, axis=1)\n",
    "\n",
    "        exports['a'] = {pairs[0] : {\"channel_names\":['PM2_5_a'],\"data\":df_a.values.tolist()}}\n",
    "        exports['b'] = {}\n",
    "        exports['avg'] = {pairs[0] : {\"channel_names\":['PM2_5'],\"data\":df_a.values.tolist()}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_v2_id(id):\n",
    "    for v2_id in v2_pairings:\n",
    "        if id in v2_pairings[v2_id]:\n",
    "            return v2_id\n",
    "    return None\n",
    "\n",
    "\n",
    "for upload in data_list:\n",
    "    # # TODO: Hack until all code is rerun\n",
    "    # if len(upload['a']) == 0:\n",
    "    #     print(\"skipping\", upload['a'].keys(), upload['b'].keys())\n",
    "    #     continue\n",
    "    # 'a' and 'b' are already paired, so we just need to look at one of them to get the v2 id\n",
    "    old_id = list(upload['a'].keys())[0]\n",
    "    v2_id = find_v2_id(old_id)\n",
    "    v2_feed = esdr.get_feed_by_id(v2_id)\n",
    "    #print(v2_feed)\n",
    "    print(old_id, v2_id)\n",
    "    # a, b, avg\n",
    "    for key in upload.keys():\n",
    "        if len(upload[key]) > 0:\n",
    "            data = list(upload[key].values())[0]\n",
    "            #print(data['channel_names'])\n",
    "            esdr.upload(v2_feed, data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88ccf14db98d8c5ffc7dfc032f8885d6bd103005d67bc8f7e90175b13672a1c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
