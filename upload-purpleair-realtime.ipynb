{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2, html, json, os, re, requests, subprocess, sys, dateutil, datetime, glob, stat\n",
    "\n",
    "from dateutil import rrule, tz, parser\n",
    "from sqlitedict import SqliteDict\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# if not os.path.exists('python-utils'):\n",
    "#     subprocess.check_output('git clone https://github.com/CMU-CREATE-Lab/python-utils.git', shell=True)\n",
    "\n",
    "# def exec_ipynb(filename_or_url):\n",
    "#     nb = (requests.get(filename_or_url).json() if re.match(r'https?:', filename_or_url) else json.load(open(filename_or_url)))\n",
    "#     if(nb['nbformat'] >= 4):\n",
    "#         src = [''.join(cell['source']) for cell in nb['cells'] if cell['cell_type'] == 'code']\n",
    "#     else:\n",
    "#         src = [''.join(cell['input']) for cell in nb['worksheets'][0]['cells'] if cell['cell_type'] == 'code']\n",
    "#     exec('\\n'.join(src), globals())\n",
    "\n",
    "# os.chdir('/t/esdr-connectors/mirror-purpleair-to-esdr/')\n",
    "# notebook_wide_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate to load utils.ipynb\n",
    "# See https://github.com/CMU-CREATE-Lab/python-utils/blob/master/utils.ipynb\n",
    "\n",
    "\n",
    "def exec_ipynb(filename_or_url):\n",
    "    nb = (requests.get(filename_or_url).json() if re.match(r'https?:', filename_or_url) else json.load(open(filename_or_url)))\n",
    "    if(nb['nbformat'] >= 4):\n",
    "        src = [''.join(cell['source']) for cell in nb['cells'] if cell['cell_type'] == 'code']\n",
    "    else:\n",
    "        src = [''.join(cell['input']) for cell in nb['worksheets'][0]['cells'] if cell['cell_type'] == 'code']\n",
    "\n",
    "    tmpname = '/tmp/%s-%s-%d.py' % (os.path.basename(filename_or_url),\n",
    "                                    datetime.datetime.now().strftime('%Y%m%d%H%M%S%f'),\n",
    "                                    os.getpid())\n",
    "    src = '\\n\\n\\n'.join(src)\n",
    "    open(tmpname, 'w').write(src)\n",
    "    code = compile(src, tmpname, 'exec')\n",
    "    exec(code, globals())\n",
    "\n",
    "\n",
    "exec_ipynb('./python-utils/utils.ipynb')\n",
    "exec_ipynb('./python-utils/esdr-library.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAT_SERVICE_NAME = 'Purpleair upload to ESDR realtime'\n",
    "STAT_HOSTNAME = 'hal21'\n",
    "STAT_SHORTNAME = 'purpleair-upload-to-esdr-realtime'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stat.set_service(STAT_SERVICE_NAME)\n",
    "progress = SqliteDict('upload-purpleair-realtime.sqlite', autocommit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(progress['lastUploaded'])\n",
    "#\n",
    "# On 2020-10-26, CPB fast-forwarded the uploader by manually setting the progress['lastUploaded'] flag to '20201021/161500utc.json.bz2', like this:\n",
    "#\n",
    "#    progress['lastUploaded'] = '20201021/161500utc.json.bz2'\n",
    "#\n",
    "# The previous value before fast-forwarding was '20200802/113000utc.json.bz2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First time uploading, create a new client like so:\n",
    "\n",
    "# Esdr.save_client('esdr-auth-purpleair-uploader.json', 'PurpleAir uploader for timemachine1')\n",
    "\n",
    "# and then follow the directions it prints, which include visiting esdr.cmucreatelab.org and creating\n",
    "# a client with given parameters, and also editing esdr-auth-baaqm-uploader.json to include your\n",
    "# username and password\n",
    "\n",
    "# Do not add esdr-auth-*.json to the git repo\n",
    "# !echo 'esdr-auth-*.json' >>.gitignore\n",
    "\n",
    "esdr = Esdr('esdr-auth-purpleair-uploader.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#\n",
    "# Parse and convert device records to ESDR\n",
    "#\n",
    "\n",
    "def computeLatLon(deviceRecord):\n",
    "    return (float(deviceRecord['Lat']), float(deviceRecord['Lon']))\n",
    "\n",
    "def computeEsdrId(deviceRecord):\n",
    "    lat, lon = computeLatLon(deviceRecord)\n",
    "\n",
    "    id = \"%s_%06d%s%06d%s\" % (deviceRecord['ID'], \n",
    "                              round(1000 * abs(lat)), 'NS'[lat < 0], \n",
    "                              round(1000 * abs(lon)), 'EW'[lon < 0])\n",
    "    return id.replace('.','_')\n",
    "\n",
    "def computeEpochTimestamp(deviceRecord):\n",
    "    try:\n",
    "        return json.loads(deviceRecord['Stats'])['lastModified'] / 1000.0\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def computeEsdrRecord(deviceRecord):\n",
    "    data = {}\n",
    "\n",
    "    data['time'] = computeEpochTimestamp(deviceRecord)\n",
    "\n",
    "    # As of 2021-01-06, we're only mirroring PM2_5, humidity, pressure, and temp_f\n",
    "    for key in ['PM2_5Value', 'humidity', 'pressure', 'temp_f']:\n",
    "        translated_key = key\n",
    "        if key == 'PM2_5Value':\n",
    "            translated_key = 'PM2_5'\n",
    "        try:\n",
    "            data[translated_key] = float(deviceRecord[key])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # As of 2021-01-06, we're no longer mirroring the stats\n",
    "    # try:\n",
    "    #     stats = json.loads(deviceRecord['Stats'])\n",
    "    #     for key in stats.keys():\n",
    "    #         if key == 'lastModified' or key == 'timeSinceModified':\n",
    "    #             continue\n",
    "    #         data['stats_' + key] = stats[key]\n",
    "    # except:\n",
    "    #     # Stats stopped being reported Jan 2018\n",
    "    #     pass\n",
    "\n",
    "    return data\n",
    "\n",
    "def computeEsdrName(deviceRecord):\n",
    "    if 'Label' in deviceRecord:\n",
    "        return deviceRecord['Label'].strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "###########################################################\n",
    "#\n",
    "# Accumulate deviceRecords from multiple JSON input files\n",
    "#\n",
    "\n",
    "def accumulateReset():\n",
    "    global accumulator\n",
    "    accumulator = defaultdict(lambda: {'records':defaultdict(lambda: {})})\n",
    "\n",
    "def accumulateJson(path):\n",
    "    nUploads = 0\n",
    "    nonameCount = 0\n",
    "    locationCounts = defaultdict(lambda:0)\n",
    "    js = json.load(bz2.open(path, 'r'))\n",
    "    records = js['results']\n",
    "    for deviceRecord in records:\n",
    "        if not ('ID' in deviceRecord):\n",
    "            locationCounts['noID'] += 1\n",
    "            continue\n",
    "\n",
    "        if not ('Lat' in deviceRecord) or not ('Lon' in deviceRecord):\n",
    "            locationCounts['noLatLon'] += 1\n",
    "            continue\n",
    "\n",
    "        epochTimestamp = computeEpochTimestamp(deviceRecord)\n",
    "        if not epochTimestamp:\n",
    "            locationCounts['noTimestamp'] += 1\n",
    "            continue\n",
    "\n",
    "        location = deviceRecord.get('DEVICE_LOCATIONTYPE','unspecified')\n",
    "        locationCounts[location] += 1\n",
    "        if location == 'inside':\n",
    "            continue\n",
    "\n",
    "        esdrId = computeEsdrId(deviceRecord)\n",
    "\n",
    "        a = accumulator[esdrId]\n",
    "        record = computeEsdrRecord(deviceRecord)\n",
    "        name = computeEsdrName(deviceRecord)\n",
    "        if not name:\n",
    "            nonameCount += 1\n",
    "            continue\n",
    "        #if 'name' in a and a['name'] != name:\n",
    "        #    raise Exception('Trying to add %s to %s, but name %s is different from %s' %\n",
    "        #                    (record, a, name, a['name']))\n",
    "        a['records'][epochTimestamp] = record\n",
    "        a['name'] = name\n",
    "        lat, lon = computeLatLon(deviceRecord)\n",
    "        a['lat'] = lat\n",
    "        a['lon'] = lon\n",
    "        nUploads += 1\n",
    "\n",
    "    Stat.info('%s: using %d of %d records %s' % (path, nUploads, len(records), json.dumps(locationCounts)), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "    Stat.info('After merge, total of %d ESDR IDs' % len(accumulator), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "    if nonameCount:\n",
    "        Stat.warning('%d purpleair records had no Label field and could not be merged' % nonameCount, host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "############################################\n",
    "#\n",
    "# Find JSON files we haven't processed yet\n",
    "#\n",
    "\n",
    "def pathTopDir(path):\n",
    "    return path.split('/')[0]\n",
    "\n",
    "def pathSansTopDir(path):\n",
    "    tokens = path.split('/')\n",
    "    return '/'.join(tokens[1:])\n",
    "\n",
    "# Get newer files than path in dir\n",
    "def getNewerFiles(dir, newerThan, nSubdirLevels):\n",
    "    ret = []\n",
    "    if nSubdirLevels:\n",
    "        for subdir in sorted(os.listdir(dir)):\n",
    "            if not newerThan or subdir > pathTopDir(newerThan):\n",
    "                for file in getNewerFiles(dir + '/' + subdir, None, nSubdirLevels-1):\n",
    "                    ret.append(subdir + '/' + file)\n",
    "            elif subdir == pathTopDir(newerThan):\n",
    "                for file in getNewerFiles(dir + '/' + subdir, pathSansTopDir(newerThan), nSubdirLevels-1):\n",
    "                    ret.append(subdir + '/' + file)\n",
    "    else:\n",
    "        for file in sorted(os.listdir(dir)):\n",
    "            if not newerThan or file > newerThan:\n",
    "                ret.append(file)\n",
    "    return ret\n",
    "\n",
    "###############################################\n",
    "#\n",
    "# Upload to ESDR\n",
    "#\n",
    "\n",
    "def sortedDict(dict):\n",
    "    return {k:dict[k] for k in sorted(dict.keys()) }\n",
    "\n",
    "def uploadId(id):\n",
    "    try:\n",
    "        a = accumulator[id]\n",
    "        if 'lat' in a and 'lon' in a:\n",
    "            feed = esdr.cached_get_or_create_product_device_feed('PurpleAir', id, a['lat'], a['lon'])\n",
    "            dicts = list(sortedDict(a['records']).values())\n",
    "            df = pd.DataFrame(dicts)\n",
    "            esdr.upload_dicts(feed, dicts)\n",
    "            #Stat.info('Successfully uploaded to feed %d' % (id), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "        else:\n",
    "            Stat.warning('Skipping ID %s because it has no lat/lon' % (id), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "    except requests.HTTPError as e:\n",
    "        Stat.warning('Failed to upload to feed corresponding to ID %s (HTTP %d)' % (id, e.response.status_code), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "    except:\n",
    "        Stat.warning('Failed to upload to feed corresponding to ID %s' % (id), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "    sys.stdout.flush()\n",
    "    sys.stderr.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this 20 times and then exit, relying on the cron job to start it up again. We do this as a simple way to deal with\n",
    "# refreshing the ESDR OAuth2 token, rather than adding code to catch the HTTP 401/403 error upon upload, yada yada. The\n",
    "# drawback of doing it this way is that the esdr.cached_get_or_create_product_device_feed() method has to start fresh\n",
    "# with a new cache, but doing so once every 20 runs is fine.\n",
    "for i in list(range(20)):\n",
    "    lastUploaded = progress.get('lastUploaded', None)\n",
    "\n",
    "    files = getNewerFiles('mirror', lastUploaded, nSubdirLevels=1)\n",
    "    # Only json.bz2 files\n",
    "    files = list(filter(re.compile(r'\\.json\\.bz2$').search, files))\n",
    "\n",
    "    # A maximum of the first 500 files\n",
    "    files = files[0:500]\n",
    "\n",
    "    accumulateReset()\n",
    "\n",
    "    for file in files:\n",
    "        accumulateJson('mirror/' + file)\n",
    "\n",
    "    for id in sorted(accumulator.keys()):\n",
    "        uploadId(id)\n",
    "\n",
    "    Stat.up('Uploaded %d devices from %d files (%s ... %s)' % (len(accumulator), len(files), files[0], files[-1]), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME,\n",
    "            valid_for_secs=3600 * 4)\n",
    "    progress['lastUploaded'] = files[-1]"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Anaconda Python3",
   "language": "python",
   "name": "anaconda3",
   "resource_dir": "/usr/local/share/jupyter/kernels/anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
